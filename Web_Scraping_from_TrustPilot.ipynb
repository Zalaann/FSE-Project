{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zalaann/FSE-Project/blob/main/Web_Scraping_from_TrustPilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JP6PLQnwDvlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba7bbc1-6187-4b41-985d-21ac4518d3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date          Author  \\\n",
            "0  2022-02-17            Catz   \n",
            "1  2022-02-17            Shea   \n",
            "2  2022-02-17         Jacquie   \n",
            "3  2022-02-17  Melanie Farmer   \n",
            "4  2022-02-17        Jennifer   \n",
            "\n",
            "                                                Body  \\\n",
            "0  Everything I've bought has been excellent, the...   \n",
            "1  I never expected such a good quality with such...   \n",
            "2                         Good clothes, good prices.   \n",
            "3  Really good product and great service n good c...   \n",
            "4  Great site for  just about anything! Plus they...   \n",
            "\n",
            "                               Heading  Rating Location  \n",
            "0     Everything I've bought has beenâ€¦       5       GB  \n",
            "1                 Best value for money       5       US  \n",
            "2                         Good clothes       5       US  \n",
            "3                           Very happy       5       GB  \n",
            "4  Great site for  just about anything       5       US  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "def trustpilot_scraper(PATH: str, n_pages):\n",
        "    # Lists to store review data\n",
        "    body = []\n",
        "    heading = []\n",
        "    rating = []\n",
        "    location = []\n",
        "    author = []\n",
        "    date = []\n",
        "\n",
        "    # Base URL for pagination\n",
        "    page = \"{}?page=\".format(PATH)\n",
        "\n",
        "    # Scrape from page 2 to n_pages (in this case 10)\n",
        "    for page_number in range(100, n_pages + 1):  # Ensure this goes up to n_pages\n",
        "        url = \"{}{}\".format(page, page_number)\n",
        "\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        req = requests.get(url, headers=headers)\n",
        "\n",
        "        # Check if request was successful\n",
        "        if req.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page_number}, status code: {req.status_code}\")\n",
        "            continue\n",
        "\n",
        "        # Adding a delay to avoid being blocked\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Parsing the page content\n",
        "        soup = BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "        # Check the raw HTML structure (for debugging purposes)\n",
        "        # Uncomment the line below to print the first page's HTML and inspect its structure\n",
        "        # print(soup.prettify())\n",
        "\n",
        "        # Extracting reviews data\n",
        "        try:\n",
        "            reviews_raw = soup.find(\"script\", id=\"__NEXT_DATA__\").string\n",
        "            reviews_raw = json.loads(reviews_raw)\n",
        "            rev = reviews_raw[\"props\"][\"pageProps\"][\"reviews\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting reviews on page {page_number}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Getting reviews into the lists\n",
        "        for i in range(len(rev)):\n",
        "            instance = rev[i]\n",
        "\n",
        "            body_ = instance[\"text\"]\n",
        "            heading_ = instance[\"title\"]\n",
        "            rating_ = instance[\"rating\"]\n",
        "            location_ = instance[\"consumer\"][\"countryCode\"]\n",
        "            author_ = instance[\"consumer\"][\"displayName\"]\n",
        "            date_ = pd.to_datetime(instance[\"dates\"][\"publishedDate\"]).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            # Appending to lists\n",
        "            body.append(body_)\n",
        "            heading.append(heading_)\n",
        "            rating.append(rating_)\n",
        "            location.append(location_)\n",
        "            author.append(author_)\n",
        "            date.append(date_)\n",
        "\n",
        "    # Creating DataFrame\n",
        "    df = {\n",
        "        'Date': date,\n",
        "        'Author': author,\n",
        "        'Body': body,\n",
        "        'Heading': heading,\n",
        "        'Rating': rating,\n",
        "        'Location': location\n",
        "    }\n",
        "\n",
        "    rev_df = pd.DataFrame(df)\n",
        "\n",
        "    # Sorting and cleaning the data\n",
        "    rev_df.sort_values(by=\"Date\", axis=0, inplace=True, ignore_index=True)\n",
        "    rev_df.drop_duplicates(subset=[\"Body\"], keep='first', inplace=True)\n",
        "    rev_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return rev_df\n",
        "\n",
        "# Usage example:\n",
        "url = \"https://www.trustpilot.com/review/www.shein.com\"\n",
        "n_pages = 500\n",
        "reviews_df = trustpilot_scraper(url, n_pages)\n",
        "\n",
        "# Optionally, save to CSV\n",
        "reviews_df.to_csv(\"trustpilot_reviews.csv\", index=True)\n",
        "\n",
        "# Show the first few rows of the dataframe\n",
        "print(reviews_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wvNf6_iiT2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}